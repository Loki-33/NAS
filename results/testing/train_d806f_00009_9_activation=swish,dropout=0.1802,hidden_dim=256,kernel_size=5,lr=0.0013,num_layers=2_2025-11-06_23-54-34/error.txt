Failure # 1 (occurred at 2025-11-06_23-54-50)
Task was killed due to the node running low on memory.
Memory on the node (IP: 10.144.151.105, ID: 1b7f78e51a87a77b1d1c05a4c328f127867e79d79040f23bbc103a51) where the lease (actor ID: NIL_IDlease ID: 090000004c8efc48d3e9e1b53a5335b28c6605ef44ca58d63d23ebb24c72fd99, name=ImplicitFunc.__init__, pid=29092, memory used=0.21GB) was running was 6.84GB / 7.14GB (0.957434), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d9bed83a4a74794a4ceb656d758610df631efcc58b3661b905bcf3fd) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.144.151.105`. To see the logs of the worker, use `ray logs worker-d9bed83a4a74794a4ceb656d758610df631efcc58b3661b905bcf3fd*out -ip 10.144.151.105. Top 10 memory users:
PID	MEM(GB)	COMMAND
3402	0.35	io.elementary.appcenter -s
29074	0.21	ray::IDLE
29092	0.21	ray::IDLE
29078	0.21	ray::IDLE
29048	0.21	ray::IDLE
29057	0.21	ray::IDLE
29056	0.21	ray::IDLE
29093	0.21	ray::IDLE
29069	0.21	ray::IDLE
29044	0.20	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
