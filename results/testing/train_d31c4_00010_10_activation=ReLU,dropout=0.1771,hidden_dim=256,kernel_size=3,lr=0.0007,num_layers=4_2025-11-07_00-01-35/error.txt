Failure # 1 (occurred at 2025-11-07_00-01-52)
Task was killed due to the node running low on memory.
Memory on the node (IP: 10.144.151.105, ID: 3a38534942cd5fd862b8289e0226b70eb92e9d71c5ce9772581fba99) where the lease (actor ID: NIL_IDlease ID: 0a000000d9889cda2b22c7d0c2bf3f92c10d23f88002c464b0268478af05ba0a, name=ImplicitFunc.__init__, pid=36760, memory used=0.22GB) was running was 6.85GB / 7.14GB (0.958533), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4f4e466f0e00f93f53521ab71b81cec02b95f62908647424693ca15) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.144.151.105`. To see the logs of the worker, use `ray logs worker-d4f4e466f0e00f93f53521ab71b81cec02b95f62908647424693ca15*out -ip 10.144.151.105. Top 10 memory users:
PID	MEM(GB)	COMMAND
36735	0.24	ray::IDLE
36745	0.23	ray::IDLE
36718	0.22	ray::IDLE
36760	0.22	ray::IDLE
36740	0.21	ray::IDLE
36725	0.21	ray::IDLE
36698	0.21	ray::IDLE
2325	0.21	firefox
36757	0.20	ray::IDLE
36722	0.20	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
