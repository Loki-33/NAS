Failure # 1 (occurred at 2025-11-06_23-42-49)
Task was killed due to the node running low on memory.
Memory on the node (IP: 10.144.151.105, ID: 570218d5abaf56db41cf41298c8537a95ae04ef630d41da0816d6253) where the lease (actor ID: NIL_IDlease ID: 11000000dfda2ce87999a1835e7198094fe2e09a5cbd72e9f61b080f1ac0688e, name=ImplicitFunc.__init__, pid=19179, memory used=0.27GB) was running was 6.87GB / 7.14GB (0.962273), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6a2574c84a427c830f35dbcd1d1832f16712a8eeddd8ad2c05e5e347) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.144.151.105`. To see the logs of the worker, use `ray logs worker-6a2574c84a427c830f35dbcd1d1832f16712a8eeddd8ad2c05e5e347*out -ip 10.144.151.105. Top 10 memory users:
PID	MEM(GB)	COMMAND
18814	0.35	ray::IDLE
19179	0.27	ray::IDLE
19177	0.25	ray::IDLE
2325	0.12	firefox
17995	0.11	ray::ImplicitFunc.train
17986	0.11	ray::ImplicitFunc.train
17996	0.11	ray::ImplicitFunc.train
18013	0.11	ray::ImplicitFunc.train
17999	0.11	ray::ImplicitFunc.train
17967	0.11	ray::ImplicitFunc.train
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
